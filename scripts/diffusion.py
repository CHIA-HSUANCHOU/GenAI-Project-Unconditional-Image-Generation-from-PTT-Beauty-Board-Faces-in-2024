# -*- coding: utf-8 -*-
"""Diffusion_submit

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mKfMDCRxJWP7Gkkw4AIOKzQ_ukbLRFkM
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, utils
from torchvision.models import inception_v3, Inception_V3_Weights
from torch.optim.lr_scheduler import CosineAnnealingLR
from PIL import Image
import glob
from tqdm import tqdm
import matplotlib.pyplot as plt
import random
import numpy as np
from google.colab import drive
from scipy import linalg
import copy

import shutil
from tqdm import tqdm
drive.mount('/content/drive')

def seed_everything(seed=2025):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True

class SimpleImageFolder(Dataset):
    def __init__(self, folder_path, image_size):
        self.image_paths = sorted(glob.glob(os.path.join(folder_path, "**", "*.png"), recursive=True))
        self.transform = transforms.Compose([
            transforms.Resize((image_size, image_size)),  # 直接 resize 成固定大小
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
        ])

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img = Image.open(self.image_paths[idx]).convert("RGB")
        return self.transform(img), 0

# 顯示前 N 張 resize 過後的圖片
def show_resized_samples(dataset_path, image_size=64, num_samples=5):
    dataset = SimpleImageFolder(dataset_path, image_size)
    plt.figure(figsize=(num_samples * 2, 2))
    for i in range(num_samples):
        img_tensor, _ = dataset[i]
        img = img_tensor.permute(1, 2, 0).cpu().numpy()  # (C,H,W) -> (H,W,C)
        img = (img * 0.5 + 0.5).clip(0, 1)  # unnormalize 回到 [0,1]
        plt.subplot(1, num_samples, i + 1)
        plt.imshow(img)
        plt.axis("off")
    plt.tight_layout()
    plt.show()

preview_dir = "/content/tmp_preview/"  # ← 最後一個斜線有時會導致某些 glob 寫法失效
if os.path.exists(preview_dir):
    shutil.rmtree(preview_dir)
os.makedirs(preview_dir, exist_ok=True)

for i, path in enumerate(sorted(glob.glob("/content/drive/MyDrive/positioned_faces/alldata/*.png"))[15:20]):
    shutil.copy(path, os.path.join(preview_dir, f"{i:03d}.png"))

show_resized_samples(preview_dir, image_size=64)

# ========== 模型定義 ==========
class SelfAttention(nn.Module):
    def __init__(self, channels, size):
        super().__init__()
        self.channels = channels
        self.size = size
        self.mha = nn.MultiheadAttention(channels, 4, batch_first=True)
        self.ln = nn.LayerNorm([channels])
        self.ff_self = nn.Sequential(
            nn.LayerNorm([channels]),
            nn.Linear(channels, channels),
            nn.GELU(),
            nn.Linear(channels, channels),
        )

    def forward(self, x):
        x = x.view(-1, self.channels, self.size * self.size).swapaxes(1, 2)
        x_ln = self.ln(x)
        attention_value, _ = self.mha(x_ln, x_ln, x_ln)
        attention_value = attention_value + x
        attention_value = self.ff_self(attention_value) + attention_value
        return attention_value.swapaxes(2, 1).view(-1, self.channels, self.size, self.size)

class DoubleConv(nn.Module):
    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):
        super().__init__()
        self.residual = residual
        if not mid_channels:
            mid_channels = out_channels
        self.double_conv = nn.Sequential(
            nn.Conv2d(in_channels, mid_channels, 3, padding=1, bias=False),
            nn.GroupNorm(1, mid_channels),
            nn.GELU(),
            nn.Conv2d(mid_channels, out_channels, 3, padding=1, bias=False),
            nn.GroupNorm(1, out_channels),
        )

    def forward(self, x):
        if self.residual:
            return F.gelu(x + self.double_conv(x))
        else:
            return self.double_conv(x)

class Down(nn.Module):
    def __init__(self, in_channels, out_channels, emb_dim=256):
        super().__init__()
        self.maxpool_conv = nn.Sequential(
            nn.MaxPool2d(2),
            DoubleConv(in_channels, in_channels, residual=True),
            DoubleConv(in_channels, out_channels),
        )
        self.emb_layer = nn.Sequential(
            nn.SiLU(),
            nn.Linear(emb_dim, out_channels)
        )

    def forward(self, x, t):
        x = self.maxpool_conv(x)
        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[2], x.shape[3])
        return x + emb

class Up(nn.Module):
    def __init__(self, in_channels, out_channels, emb_dim=256):
        super().__init__()
        self.up = nn.Upsample(scale_factor=2, mode="bilinear", align_corners=True)
        self.conv = nn.Sequential(
            DoubleConv(in_channels, in_channels, residual=True),
            DoubleConv(in_channels, out_channels, in_channels // 2),
        )
        self.emb_layer = nn.Sequential(
            nn.SiLU(),
            nn.Linear(emb_dim, out_channels)
        )

    def forward(self, x, skip_x, t):
        x = self.up(x)
        x = torch.cat([skip_x, x], dim=1)
        x = self.conv(x)
        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[2], x.shape[3])
        return x + emb

class StrongerUNet(nn.Module):
    def __init__(self, c_in=3, c_out=3, time_dim=256, device="cuda"):
        super().__init__()
        self.device = device
        self.time_dim = time_dim

        self.inc = DoubleConv(c_in, 128)  # 原本64，擴大初始通道

        self.down1 = Down(128, 256)
        self.sa1 = SelfAttention(256,32)

        self.down2 = Down(256, 384)
        self.sa2 = SelfAttention(384,16)

        self.down3 = Down(384, 512)
        self.sa3 = SelfAttention(512,8)

        self.bot1 = DoubleConv(512, 768)
        self.bot2 = DoubleConv(768, 768)
        self.bot3 = DoubleConv(768, 512)

        self.up1 = Up(896, 384)
        self.sa4 = SelfAttention(384, 16)

        self.up2 = Up(640, 256)
        self.sa5 = SelfAttention(256, 32)

        self.up3 = Up(384, 128)
        self.sa6 = SelfAttention(128, 64)

        self.outc = nn.Conv2d(128, c_out, kernel_size=1)

    def pos_encoding(self, t, channels):
        inv_freq = 1.0 / (10000 ** (torch.arange(0, channels, 2, device=self.device).float() / channels))
        pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)
        pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)
        return torch.cat([pos_enc_a, pos_enc_b], dim=-1)

    def forward(self, x, t):
        t = t.unsqueeze(-1).float()
        t = self.pos_encoding(t, self.time_dim)

        x1 = self.inc(x)
        x2 = self.down1(x1, t)
        x2 = self.sa1(x2)

        x3 = self.down2(x2, t)
        x3 = self.sa2(x3)

        x4 = self.down3(x3, t)
        x4 = self.sa3(x4)

        x4 = self.bot1(x4)
        x4 = self.bot2(x4)
        x4 = self.bot3(x4)

        x = self.up1(x4, x3, t)
        x = self.sa4(x)
        x = self.up2(x, x2, t)
        x = self.sa5(x)
        x = self.up3(x, x1, t)
        x = self.sa6(x)

        return self.outc(x)

# ========== Diffusion ==========
class Diffusion:
    def __init__(self, noise_steps=1000, beta_start=1e-4, beta_end=0.02, img_size=64, device="cuda"):
        self.noise_steps = noise_steps
        self.beta = torch.linspace(beta_start, beta_end, noise_steps).to(device)
        self.alpha = 1. - self.beta
        self.alpha_hat = torch.cumprod(self.alpha, dim=0)
        self.img_size = img_size
        self.device = device

    def noise_images(self, x, t):
        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]
        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]
        epsilon = torch.randn_like(x)
        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * epsilon, epsilon

    def sample_timesteps(self, n):
        return torch.randint(low=1, high=self.noise_steps, size=(n,)).to(self.device)

    def sample(self, model, n, seed=2025):
        model.eval()
        x = torch.randn((n, 3, self.img_size, self.img_size)).to(self.device)
        for i in tqdm(reversed(range(1, self.noise_steps)), desc='Sampling'):
            t = torch.full((n,), i, device=self.device, dtype=torch.long)
            predicted_noise = model(x, t)
            alpha = self.alpha[t][:, None, None, None]
            alpha_hat = self.alpha_hat[t][:, None, None, None]
            beta = self.beta[t][:, None, None, None]
            noise = torch.randn_like(x) if i > 1 else torch.zeros_like(x)
            x = (1 / torch.sqrt(alpha)) * (x - ((1 - alpha) / torch.sqrt(1 - alpha_hat)) * predicted_noise) + torch.sqrt(beta) * noise
        x = (x.clamp(-1, 1) + 1) / 2
        return x

# ========== Save Sample Images ==========
def save_images(images, path, nrow=4):
    from torchvision.utils import make_grid
    grid = make_grid(images, nrow=nrow)
    ndarr = grid.permute(1, 2, 0).cpu().numpy()
    im = Image.fromarray((ndarr * 255).astype('uint8'))
    os.makedirs(os.path.dirname(path), exist_ok=True)
    im.save(path)

"""## 訓練
"""

class EMA:
    def __init__(self, beta):
        self.beta = beta
        self.step = 0

    def update_model_average(self, ma_model, current_model):
        for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):
            old_weight, up_weight = ma_params.data, current_params.data
            ma_params.data = self.update_average(old_weight, up_weight)

    def update_average(self, old, new):
        if old is None:
            return new
        return old * self.beta + (1 - self.beta) * new

    def step_ema(self, ema_model, model, step_start_ema=100):
        if self.step == 0:
        # 第一步先對齊一次參數
            print("[EMA] Initializing EMA model parameters.")
            self.reset_parameters(ema_model, model)
        elif self.step >= step_start_ema:
        # 之後才開始 EMA 平滑更新
            self.update_model_average(ema_model, model)
        self.step += 1

    def reset_parameters(self, ema_model, model):
        ema_model.load_state_dict(model.state_dict())

def debug_sample(model, diffusion, dataset, device="cuda"):
    model.eval()
    img, _ = dataset[0]
    img = img.unsqueeze(0).to(device)

    t = diffusion.sample_timesteps(1)
    x_t, noise = diffusion.noise_images(img, t)
    predicted = model(x_t, t)

    alpha = diffusion.alpha[t][:, None, None, None]
    alpha_hat = diffusion.alpha_hat[t][:, None, None, None]
    x_denoised = (1 / torch.sqrt(alpha)) * (
        x_t - ((1 - alpha) / torch.sqrt(1 - alpha_hat)) * predicted
    )

    x_denoised = x_denoised.clamp(-1, 1).cpu()
    x_denoised = (x_denoised + 1) / 2
    x_t = (x_t.cpu() + 1) / 2

    print(f"[DEBUG] timestep: {t.item()}")
    print(f"[INFO] x_t range: {x_t.min().item():.4f} ~ {x_t.max().item():.4f}")
    print(f"[INFO] predicted noise mean: {predicted.mean().item():.4f}, std: {predicted.std().item():.4f}")
    print(f"[INFO] x_denoised range: {x_denoised.min().item():.4f} ~ {x_denoised.max().item():.4f}")

def train_model(run_name, dataset_path, image_size=64, batch_size=16, epochs=5, lr=3e-4,
                device="cuda", seed=2025, noise_steps=1000, beta_start=1e-4, beta_end=0.02,
                save_every=5):
    rgb_means = []  # 新增一個列表來儲存每個 epoch 的 RGB mean
    seed_everything(seed)
    dataset = SimpleImageFolder(dataset_path, image_size)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)

    model = StrongerUNet(device=device).to(device)
    diffusion = Diffusion(noise_steps=noise_steps, beta_start=beta_start, beta_end=beta_end,
                          img_size=image_size, device=device)
    ema = EMA(0.998)
    ema_model = copy.deepcopy(model).eval().requires_grad_(False)

    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
    scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-5)

    mse = nn.MSELoss()

    os.makedirs(f"models/{run_name}", exist_ok=True)
    os.makedirs(f"results/{run_name}", exist_ok=True)

    num_batches_for_ema = 3

    train_losses = []
    ema_losses = []
    for epoch in range(epochs):
        print(f"\nEpoch {epoch+1}")
        model.train()
        epoch_loss = 0
        pbar = tqdm(dataloader)

        last_x_t, last_t, last_noise = [], [], []  
        for i, (images, _) in enumerate(pbar):
            images = images.to(device)
            t = diffusion.sample_timesteps(images.size(0))
            x_t, noise = diffusion.noise_images(images, t)

            predicted = model(x_t, t)
            loss = mse(predicted, noise)
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            ema.step_ema(ema_model, model)

            epoch_loss += loss.item()
            if i >= len(dataloader) - num_batches_for_ema:
                last_x_t.append(x_t.detach())
                last_t.append(t.detach())
                last_noise.append(noise.detach())

            pbar.set_postfix(MSE=loss.item())

        avg_loss = epoch_loss / len(dataloader)
        train_losses.append(avg_loss)  
        print(f" Epoch {epoch+1} 平均 Loss: {avg_loss:.4f}")
        scheduler.step()
        print(f"[INFO] Learning rate: {scheduler.get_last_lr()[0]:.6f}")

        ema_model.eval()
        ema_loss_total = 0
        with torch.no_grad():
            for xti, ti, nsei in zip(last_x_t, last_t, last_noise):
                ema_pred = ema_model(xti, ti)
                ema_loss_total += mse(ema_pred, nsei).item()
        ema_loss = ema_loss_total / num_batches_for_ema
        ema_losses.append(ema_loss)
        print(f"EMA Loss: {ema_loss:.4f}")
        del last_x_t, last_t, last_noise  

        if ((epoch + 1) <= 15 and (epoch + 1) % 5 == 0) or ((epoch + 1) > 15 and (epoch + 1) % 10 == 0):
            debug_sample(ema_model, diffusion, dataset, device=device)

        if epoch + 1 >= 10 and (epoch + 1) % 5 == 0:
            model.eval()
            with torch.no_grad():
                #sampled = diffusion.sample(model, n=16)
                ema_sampled_images = diffusion.sample(ema_model, n=16)
                #save_images(sampled, f"results/{run_name}/epoch_{epoch+1}.png")
                save_images(ema_sampled_images, f"results/{run_name}/epoch_{epoch+1}_ema.png")

                rgb_mean = ema_sampled_images.mean(dim=(0, 2, 3))
                rgb_means.append(rgb_mean.detach().cpu().numpy())  # 儲存到列表
                print(f"[INFO] RGB mean at epoch {epoch+1}: R={rgb_mean[0]:.3f}, G={rgb_mean[1]:.3f}, B={rgb_mean[2]:.3f}")
            sampled_show = ema_sampled_images[:8]
            grid = utils.make_grid(sampled_show, nrow=4).permute(1, 2, 0).cpu().numpy()
            grid = (grid * 255).astype("uint8")
            plt.figure(figsize=(8, 4))
            plt.imshow(grid)
            plt.axis("off")
            plt.title(f"Generated at Epoch {epoch+1}")
            plt.show()

        if (epoch + 1) % save_every == 0 or epoch == epochs - 1:
            torch.save({
                "model": model.state_dict(),
                "optimizer": optimizer.state_dict(),
                "scheduler": scheduler.state_dict(),
                "epoch": epoch + 1,
                "ema_model": ema_model.state_dict()
                }, f"models/{run_name}/epoch_{epoch}.pt")


# ========== 訓練結束後畫 loss 曲線 ==========
    plt.figure(figsize=(8, 4))
    plt.plot(range(1, epochs + 1), train_losses, label="Train Loss")
    plt.xlabel("Epoch")
    plt.ylabel("MSE Loss")
    plt.title("Training Loss Curve")
    plt.grid(True)
    plt.legend()
    plt.show()
    # ========== 訓練結束後畫 EMA Loss 曲線 ==========
    plt.figure(figsize=(8, 4))
    plt.plot(range(1, epochs + 1), ema_losses, label="EMA Loss", color='orange')
    plt.xlabel("Epoch")
    plt.ylabel("MSE Loss")
    plt.title("EMA Model Loss Curve")
    plt.grid(True)
    plt.legend()
    plt.show()

        # ========== 訓練結束後畫 RGB mean 曲線 ==========
    if rgb_means:
       rgb_means = np.array(rgb_means)
       x_ticks = list(range(10, 10 + 5 * len(rgb_means), 5))  # 10, 15, ..., 對應你儲存的 epoch

       plt.figure(figsize=(8, 4))
       plt.plot(x_ticks, rgb_means[:, 0], label="R mean")
       plt.plot(x_ticks, rgb_means[:, 1], label="G mean")
       plt.plot(x_ticks, rgb_means[:, 2], label="B mean")
       plt.axhline(0.5, color='gray', linestyle='--', label="Ideal (0.5)")
       plt.title("RGB Mean of Generated Images")
       plt.xlabel("Epoch")
       plt.ylabel("Channel Mean")
       plt.legend()
       plt.grid(True)
       plt.show()

# ========== 開始訓練 ==========
train_model(
    run_name="ddpm_3",
    dataset_path="/content/drive/MyDrive/positioned_faces/alldata",
    image_size=64,  
    batch_size=16,
    epochs=160,
    lr=3e-4,
    device="cuda" if torch.cuda.is_available() else "cpu",
    seed=2025,
    noise_steps=1000,
    beta_start=1e-4,
    beta_end=0.02,
)


import gc
import torch

gc.collect()
torch.cuda.empty_cache()


"""#### 選110epoch生成10000張=>42.99"""

# epoch = 110
def generate_images(model_ckpt, output_dir, image_size=64, total_images=10000, batch_size=100, device="cuda", seed=2025):
    seed_everything(seed)
    os.makedirs(output_dir, exist_ok=True)

    model = StrongerUNet(device=device).to(device)
    ckpt = torch.load(model_ckpt, map_location=device)
    model.load_state_dict(ckpt["ema_model"])
    model.eval()

    diffusion = Diffusion(img_size=image_size, device=device)

    saved_count = 0
    idx = 0

    with torch.no_grad():
        while saved_count < total_images:
            n = min(batch_size, total_images - saved_count)
            sampled = diffusion.sample(model, n=n)
            sampled = (sampled * 255).clamp(0, 255).byte()

            for j in range(sampled.size(0)):
                img_tensor = sampled[j]
                img = transforms.ToPILImage()(img_tensor.cpu())
                img.save(os.path.join(output_dir, f"{idx:05d}.png"))
                idx += 1
                saved_count += 1

    print(f"✅ 實際產生 {saved_count} 張圖片（已自動略過過曝）")
generate_images(
    model_ckpt="/content/drive/MyDrive/ddpm3_checkpoints/epoch_109.pt",        # ← 你指定的模型檔案
    output_dir="generated_images",   # ← 輸出資料夾
    image_size=64,
    total_images=10000,
    batch_size=85,
    device="cuda" if torch.cuda.is_available() else "cpu"
    )


def backup_generated_images(output_dir="generated_images", final_backup_dir="/content/drive/MyDrive/ddpm3test5"):
    os.makedirs(final_backup_dir, exist_ok=True)
    for fname in tqdm(os.listdir(output_dir), desc="Copying to Google Drive"):
        src_path = os.path.join(output_dir, fname)
        dst_path = os.path.join(final_backup_dir, fname)
        shutil.copy2(src_path, dst_path)  # copy2 保留 metadata

    print(f"✅ 所有圖片已複製到 {final_backup_dir}")

backup_generated_images()

# ==== 路徑設定（你可以根據實際情況修改）====
IMG_DIR = "/content/generated_images"  # ← 你產生的圖片資料夾
REF_MU_PATH = "/content/drive/MyDrive/Evaluation/test_mu.npy"
REF_SIGMA_PATH = "/content/drive/MyDrive/Evaluation/test_sigma.npy"
BATCH_SIZE = 32
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def get_inception_model(device):
    weights = Inception_V3_Weights.IMAGENET1K_V1
    model = inception_v3(weights=weights, transform_input=False)
    model.fc = torch.nn.Identity()
    model.to(device)
    model.eval()
    return model


def get_image_transform():
    return transforms.Compose([
        transforms.Resize((299, 299)),
        transforms.ToTensor(),
        transforms.Normalize([0.5]*3, [0.5]*3)
    ])


def extract_features(img_dir, model, transform, device, batch_size=32):
    files = [os.path.join(img_dir, f) for f in os.listdir(img_dir)
             if f.lower().endswith((".jpg", ".png", ".jpeg"))]
    if len(files) == 0:
        raise ValueError(f"No image files found in {img_dir}")

    features = []
    with torch.no_grad():
        for i in tqdm(range(0, len(files), batch_size), desc="Extracting features"):
            batch = []
            for f in files[i:i + batch_size]:
                try:
                    img = Image.open(f).convert('RGB')
                    batch.append(transform(img))
                except Exception as e:
                    print(f"[WARNING] Failed to load image: {f} ({e})")
            if not batch:
                continue
            batch = torch.stack(batch).to(device)
            pred = model(batch)
            features.append(pred.cpu().numpy())

    if not features:
        raise ValueError("No valid features extracted. Check image files or transform.")

    return np.concatenate(features, axis=0)


def calculate_activation_statistics(features):
    mu = np.mean(features, axis=0)
    sigma = np.cov(features, rowvar=False)
    return mu, sigma


def calculate_fid(mu1, sigma1, mu2, sigma2):
    covmean, _ = linalg.sqrtm(sigma1 @ sigma2, disp=False)
    if np.iscomplexobj(covmean):
        covmean = covmean.real
    diff = mu1 - mu2
    fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)
    return fid


# ==== 執行 ====
print(f"[INFO] Using device: {DEVICE}")
model = get_inception_model(DEVICE)
transform = get_image_transform()

features = extract_features(IMG_DIR, model, transform, DEVICE, batch_size=BATCH_SIZE)
mu_fake, sigma_fake = calculate_activation_statistics(features)

mu_real = np.load(REF_MU_PATH)
sigma_real = np.load(REF_SIGMA_PATH)

fid_score = calculate_fid(mu_real, sigma_real, mu_fake, sigma_fake)
print(f"\n✅ FID score = {fid_score:.4f}")

import shutil

# 將 generated_images 資料夾壓縮成 zip
shutil.make_archive("generated_images", "zip", "generated_images")

print("✅ 壓縮完成，可下載 generated_images.zip 檔案")

from google.colab import files

files.download("/content/generated_images.zip")
print("✅ 預測完成，generated_images.zip 已儲存並下載。")



"""#### 有schelduer的resume=>跑到180"""

def debug_sample(model, diffusion, dataset, device="cuda"):
    model.eval()
    img, _ = dataset[0]
    img = img.unsqueeze(0).to(device)

    t = diffusion.sample_timesteps(1)
    x_t, noise = diffusion.noise_images(img, t)
    predicted = model(x_t, t)

    alpha = diffusion.alpha[t][:, None, None, None]
    alpha_hat = diffusion.alpha_hat[t][:, None, None, None]
    x_denoised = (1 / torch.sqrt(alpha)) * (
        x_t - ((1 - alpha) / torch.sqrt(1 - alpha_hat)) * predicted
    )

    x_denoised = x_denoised.clamp(-1, 1).cpu()
    x_denoised = (x_denoised + 1) / 2
    x_t = (x_t.cpu() + 1) / 2

    print(f"[DEBUG] timestep: {t.item()}")
    print(f"[INFO] x_t range: {x_t.min().item():.4f} ~ {x_t.max().item():.4f}")
    print(f"[INFO] predicted noise mean: {predicted.mean().item():.4f}, std: {predicted.std().item():.4f}")
    print(f"[INFO] x_denoised range: {x_denoised.min().item():.4f} ~ {x_denoised.max().item():.4f}")

def load_checkpoint(ckpt_path, model, optimizer=None, scheduler=None, ema_model=None):
    checkpoint = torch.load(ckpt_path)
    model.load_state_dict(checkpoint["model"])
    if optimizer and "optimizer" in checkpoint:
        optimizer.load_state_dict(checkpoint["optimizer"])
    if scheduler and "scheduler" in checkpoint:
        scheduler.load_state_dict(checkpoint["scheduler"])
    if ema_model and "ema_model" in checkpoint:
        ema_model.load_state_dict(checkpoint["ema_model"])
    start_epoch = checkpoint.get("epoch", 0)
    return model, optimizer, scheduler, ema_model, start_epoch

def resume_train_model(run_name, dataset_path, image_size=64, batch_size=16, total_epochs=90,
                       lr=3e-4, device="cuda", seed=2025, noise_steps=1000, beta_start=1e-4,
                       beta_end=0.02, save_every=5, resume_ckpt=None):

    rgb_means = []
    seed_everything(seed)
    dataset = SimpleImageFolder(dataset_path, image_size)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)

    model = StrongerUNet(device=device).to(device)
    diffusion = Diffusion(noise_steps=noise_steps, beta_start=beta_start, beta_end=beta_end,
                          img_size=image_size, device=device)
    ema = EMA(0.998)
    ema_model = copy.deepcopy(model).eval().requires_grad_(False)
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)

    mse = nn.MSELoss()

    start_epoch = 0
    scheduler = CosineAnnealingLR(optimizer, T_max=total_epochs, eta_min=1e-5)  # 預設
    if resume_ckpt:
        model, optimizer, scheduler, ema_model, start_epoch = load_checkpoint(
            resume_ckpt, model, optimizer, scheduler, ema_model
        )
        scheduler = CosineAnnealingLR(optimizer, T_max=total_epochs - start_epoch, eta_min=1e-5)
        print(f"[INFO] Resuming from epoch {start_epoch}")

    print(f"[CHECK] Training will continue from epoch {start_epoch} to {total_epochs - 1}")
    os.makedirs(f"models/{run_name}", exist_ok=True)
    os.makedirs(f"results/{run_name}", exist_ok=True)

    for epoch in range(start_epoch, total_epochs):
        print(f"\nEpoch {epoch+1}")
        model.train()
        epoch_loss = 0
        pbar = tqdm(dataloader)

        for i, (images, _) in enumerate(pbar):
            images = images.to(device)
            t = diffusion.sample_timesteps(images.size(0))
            x_t, noise = diffusion.noise_images(images, t)

            predicted = model(x_t, t)
            loss = mse(predicted, noise)

            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            ema.step_ema(ema_model, model)

            epoch_loss += loss.item()
            pbar.set_postfix(MSE=loss.item())

        avg_loss = epoch_loss / len(dataloader)
        print(f"✅ Epoch {epoch+1} 平均 Loss: {avg_loss:.4f}")

        if ((epoch + 1) <= 15 and (epoch + 1) % 5 == 0) or ((epoch + 1) > 15 and (epoch + 1) % 10 == 0):
            debug_sample(ema_model, diffusion, dataset, device=device)

        if epoch + 1 >= 10 and (epoch + 1) % 5 == 0:
            model.eval()
            with torch.no_grad():
                ema_sampled_images = diffusion.sample(ema_model, n=16)
                save_images(ema_sampled_images, f"results/{run_name}/epoch_{epoch+1}_ema.png")

                rgb_mean = ema_sampled_images.mean(dim=(0, 2, 3))
                rgb_means.append(rgb_mean.detach().cpu().numpy())
                print(f"[INFO] RGB mean at epoch {epoch+1}: R={rgb_mean[0]:.3f}, G={rgb_mean[1]:.3f}, B={rgb_mean[2]:.3f}")

        if (epoch + 1) % save_every == 0 or epoch == total_epochs - 1:
            torch.save({
                "model": model.state_dict(),
                "optimizer": optimizer.state_dict(),
                "scheduler": scheduler.state_dict(),
                "epoch": epoch + 1,
                "ema_model": ema_model.state_dict()
            }, f"models/{run_name}/epoch_{epoch}.pt")

resume_train_model(
    run_name="ddpm_3",
    dataset_path="/content/drive/MyDrive/positioned_faces/alldata",
    image_size=64,
    batch_size=16,
    total_epochs=180, 
    lr=3e-4,
    device="cuda",
    seed=2025,
    resume_ckpt="/content/drive/MyDrive/ddpm3_checkpoints/epoch_159.pt"
)
